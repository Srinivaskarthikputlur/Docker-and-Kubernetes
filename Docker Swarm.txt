
Swarm Cluster may contain multiple nodes
We can add any number of nodes to a cluster. 
Cluster contains "manager" and "worker" nodes.
The node where swarm cluster is initialised becomes a manager node.
Swarm is initialised using "docker swarm init"
Workers nodes need tokens to join a Swarm cluster. These tokens are shown while initialising a swarm using "docker swarm init" on the manager node.
Services created from any node have replicas(Containers/tasks) running on multiple nodes. They run at cluster level.
Services can be accessed from any nodes.



********************************************************************************************************
//command to check if swarm is active.
λ docker info

//to initialise swarm
#docker swarm init

//to stop the swarm service
C:\Users\nspr2
λ docker swarm leave
Error response from daemon: You are attempting to leave the swarm on a node that is participating as a manager. Removing the last manager erases all current state of the swarm. Use `--force` to ignore this message.

C:\Users\nspr2
λ docker swarm leave --force
Node left the swarm.

//to initialise swarm
C:\Users\nspr2                                                                                                                                                                                               λ docker swarm init
Error response from daemon: could not choose an IP address to advertise since this system has multiple addresses on different interfaces (10.58.132.6 on eth0 and 192.168.99.100 on eth1) - specify one with --advertise-addr  

C:\Users\nspr2
λ docker swarm init --advertise-addr 192.168.99.100

C:\Users\nspr2
λ docker swarm init --advertise-addr 192.168.99.100
Swarm initialized: current node (35pr9qa7vzt2mn6grobd9zo13) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-33j6lkof27c8eunhgh9hu7uf7s9xa3dy6wtw7kp61vji72ybtb-eq2m22tf6rq73dd58fg58gp7x 192.168.99.100:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.

****************************docker node command************************************

C:\Users\nspr2
λ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
gdnl0wwntj6b2xy0toogq9esf *   default             Ready               Active              Leader              18.09.4

//The above is the manager node which is a leader. There can only be one leader at a time.


// docker node commands
C:\Users\nspr2
λ docker node --help

Usage:  docker node COMMAND

Manage Swarm nodes

Options:


Commands:
  demote      Demote one or more nodes from manager in the swarm
  inspect     Display detailed information on one or more nodes
  ls          List nodes in the swarm
  promote     Promote one or more nodes to manager in the swarm
  ps          List tasks running on one or more nodes, defaults to current node
  rm          Remove one or more nodes from the swarm
  update      Update a node

Run 'docker node COMMAND --help' for more information on a command.



**************************************docker service command*************************************

//service in the swarm replaces the docker run.

C:\Users\nspr2
λ docker service --help

Usage:  docker service COMMAND

Manage services

Options:


Commands:
  create      Create a new service
  inspect     Display detailed information on one or more services
  logs        Fetch the logs of a service or task
  ls          List services
  ps          List the tasks of one or more services
  rm          Remove one or more services
  rollback    Revert changes to a service's configuration
  scale       Scale one or multiple replicated services
  update      Update a service

Run 'docker service COMMAND --help' for more information on a command.



***************************Create a service using an alpine image*******************************************************

C:\Users\nspr2
λ docker service create alpine ping 8.8.8.8
vrjvk2rl7cbyefn6s1k8aog0o
overall progress: 1 out of 1 tasks
1/1: running   [==================================================>]
verify: Service converged


// list the services
C:\Users\nspr2
λ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
vrjvk2rl7cby        agitated_minsky     replicated          1/1                 alpine:latest

// To view the containers of the above service use docker service ps <service_name>
C:\Users\nspr2
λ docker service ps agitated_minsky
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
scmsydu9m7nt        agitated_minsky.1   alpine:latest       default             Running             Running 4 minutes ago

//You will notice that the container is running on a node - default.

C:\Users\nspr2
λ docker container ls
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
132ef7c22d66        alpine:latest       "ping 8.8.8.8"      7 minutes ago       Up 8 minutes                            agitated_minsky.1.scmsydu9m7ntn0lnzte14uhy7


**************************************To scale up a service*******************************************
//docker service update <service_name> --replicas <number to scale>
***************************************************************************************************

C:\Users\nspr2
λ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
vrjvk2rl7cby        agitated_minsky     replicated          1/1                 alpine:latest

C:\Users\nspr2
λ docker service update vrjvk2rl7cby --replicas 3
vrjvk2rl7cby
overall progress: 3 out of 3 tasks
1/3: running   [==================================================>]
2/3: running   [==================================================>]
3/3: running   [==================================================>]
verify: Service converged

//Notice the REPLICAS column. There are 3 now.
C:\Users\nspr2
λ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
vrjvk2rl7cby        agitated_minsky     replicated          3/3                 alpine:latest


//To see the tasks/containers in the service

C:\Users\nspr2
λ docker service ps agitated_minsky
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
scmsydu9m7nt        agitated_minsky.1   alpine:latest       default             Running             Running 16 minutes ago
jzuqxeu57rio        agitated_minsky.2   alpine:latest       default             Running             Running 3 minutes ago
hcqnidcemdym        agitated_minsky.3   alpine:latest       default             Running             Running 3 minutes ago

C:\Users\nspr2
λ docker container ls
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
f27293b45378        alpine:latest       "ping 8.8.8.8"      4 minutes ago       Up 4 minutes                            agitated_minsky.2.jzuqxeu57rioc7tpni4hmreti
bf2fec2fb60d        alpine:latest       "ping 8.8.8.8"      4 minutes ago       Up 4 minutes                            agitated_minsky.3.hcqnidcemdymm0j6pyn5tqftj
132ef7c22d66        alpine:latest       "ping 8.8.8.8"      17 minutes ago      Up 17 minutes                           agitated_minsky.1.scmsydu9m7ntn0lnzte14uhy7


**************************************************************************************************************************************************
*******************************************************Swarm orchestration****************************
**************************************************************************************************************************************************
//Try deleting a running container. Swarm will automatically spin up one for you.


//remove the first container out of 3.
C:\Users\nspr2
λ docker container rm -f agitated_minsky.1.scmsydu9m7ntn0lnzte14uhy7
agitated_minsky.1.scmsydu9m7ntn0lnzte14uhy7


//REPLICAS became '2/3'
C:\Users\nspr2
λ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
vrjvk2rl7cby        agitated_minsky     replicated          2/3                 alpine:latest

//SWARM spinned up another container
C:\Users\nspr2
λ docker service ps vrjvk2rl7cby
ID                  NAME                    IMAGE               NODE                DESIRED STATE       CURRENT STATE                    ERROR                         PORTS
fxzjucva6bt3        agitated_minsky.1       alpine:latest       default             Running             Running less than a second ago
scmsydu9m7nt         \_ agitated_minsky.1   alpine:latest       default             Shutdown            Failed less than a second ago    "task: non-zero exit (137)"
jzuqxeu57rio        agitated_minsky.2       alpine:latest       default             Running             Running 21 minutes ago
hcqnidcemdym        agitated_minsky.3       alpine:latest       default             Running             Running 21 minutes ago

C:\Users\nspr2
λ docker container ls
CONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS               NAMES
889bf8e9c786        alpine:latest       "ping 8.8.8.8"      Less than a second ago   Up 20 seconds                           agitated_minsky.1.fxzjucva6bt3j9210ovifsuh4
f27293b45378        alpine:latest       "ping 8.8.8.8"      21 minutes ago           Up 22 minutes                           agitated_minsky.2.jzuqxeu57rioc7tpni4hmreti
bf2fec2fb60d        alpine:latest       "ping 8.8.8.8"      21 minutes ago           Up 22 minutes                           agitated_minsky.3.hcqnidcemdymm0j6pyn5tqftj

//REPLCIAS are 3/3 again.
C:\Users\nspr2
λ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
vrjvk2rl7cby        agitated_minsky     replicated          3/3                 alpine:latest


*******************************************************************************************************************************************************************
**************To delete containers, you need to delete the service******************
*******************************************************************************************************************************************************************

C:\Users\nspr2
λ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
vrjvk2rl7cby        agitated_minsky     replicated          3/3                 alpine:latest


//DELETE the service
C:\Users\nspr2
λ docker service rm vrjvk2rl7cby
vrjvk2rl7cby

//SERVICE deleted.
C:\Users\nspr2
λ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS

//No containers running
C:\Users\nspr2
λ docker container ls
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES

//No service running.
C:\Users\nspr2
λ docker service ps vrjvk2rl7cby
no such service: vrjvk2rl7cby

C:\Users\nspr2
λ docker container ls
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES

*******************************************************************************************************************************************************************
************************************************Docker 3-Node Swarm using play-with-docker.com site
Manager nodes and Worker nodes.
********************************************************************************************************************************************************************
Create three nodes.

//Node-1
[node1] (local) root@192.168.0.48 ~
$ 

//Node-2
###############################################################
#                          WARNING!!!!                        #
# This is a sandbox environment. Using personal credentials   #
# is HIGHLY! discouraged. Any consequences of doing so are    #
# completely the user's responsibilites.                      #
#                                                             #
# The PWD team.                                               #
###############################################################
[node2] (local) root@192.168.0.47 ~
$ 

//Node-3
###############################################################
#                          WARNING!!!!                        #
# This is a sandbox environment. Using personal credentials   #
# is HIGHLY! discouraged. Any consequences of doing so are    #
# completely the user's responsibilites.                      #
#                                                             #
# The PWD team.                                               #
###############################################################
[node3] (local) root@192.168.0.46 ~
$ 

************************************************************************************
Initialise swarm on node-1

[node1] (local) root@192.168.0.48 ~
$ docker swarm init --advertise-addr  192.168.0.48
Swarm initialized: current node (85wy3so1zksl8zdh12jf65s13) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-19ipkr990g4rajyictoz6czytxh9c4ua5g2xv0m9q24p73ax2n-blix3aa8k8nwc3ln1m5tky4gl 192.168.0.48:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.

[node1] (local) root@192.168.0.48 ~
$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
85wy3so1zksl8zdh12jf65s13 *   node1               Ready               Active              Leader              19.03.4
[node1] (local) root@192.168.0.48 ~
$ 


//You will notice that node-1 is the manager.

//To make node-2 join the swarm, copy the below command in node-2
//docker swarm join --token SWMTKN-1-19ipkr990g4rajyictoz6czytxh9c4ua5g2xv0m9q24p73ax2n-blix3aa8k8nwc3ln1m5tky4gl 192.168.0.48:2377

[node2] (local) root@192.168.0.47 ~
$ docker swarm join --token SWMTKN-1-19ipkr990g4rajyictoz6czytxh9c4ua5g2xv0m9q24p73ax2n-blix3aa8k8nwc3ln1m5tky4gl 192.168.0.48:2377
This node joined a swarm as a worker.
[node2] (local) root@192.168.0.47 ~
$ 

//You cannot see the list of nodes from a worker. You can check that only from a manager node.
$ docker node ls
Error response from daemon: This node is not a swarm manager. Worker nodes can't be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager.
[node2] (local) root@192.168.0.47 ~
$ 

****************************************************************************************
//Check if node-2 has joined the swarm. Type the below from node-1 which is the manager.


[node1] (local) root@192.168.0.48 ~
$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
85wy3so1zksl8zdh12jf65s13 *   node1               Ready               Active              Leader              19.03.4
iqcag9pn0sfvzrghzno0su04e     node2               Ready               Active                                  19.03.4
[node1] (local) root@192.168.0.48 ~
$ 



*******************************************************************************************

Add node-3 to the swarm cluster and check the node status from node-1

[node3] (local) root@192.168.0.46 ~
$ docker swarm join --token SWMTKN-1-19ipkr990g4rajyictoz6czytxh9c4ua5g2xv0m9q24p73ax2n-blix3aa8k8nwc3ln1m5tky4gl 192.168.0.48:2377
This node joined a swarm as a worker.
[node3] (local) root@192.168.0.46 ~
$ 

[node1] (local) root@192.168.0.48 ~
$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
85wy3so1zksl8zdh12jf65s13 *   node1               Ready               Active              Leader              19.03.4
iqcag9pn0sfvzrghzno0su04e     node2               Ready               Active                                  19.03.4
f0e21cirlqfj7undr2v68brd7     node3               Ready               Active                                  19.03.4
[node1] (local) root@192.168.0.48 ~
$ 

*****************************************************************************************************************************************************
********************************************Promote a worker node to manager node************************************************************************
**************************************************************************************************************************************************

[node1] (local) root@192.168.0.48 ~
$ docker node update --role manager node2
node2
[node1] (local) root@192.168.0.48 ~
$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
85wy3so1zksl8zdh12jf65s13 *   node1               Ready               Active              Leader              19.03.4
iqcag9pn0sfvzrghzno0su04e     node2               Ready               Active              Reachable           19.03.4
f0e21cirlqfj7undr2v68brd7     node3               Ready               Active                                  19.03.4
[node1] (local) root@192.168.0.48 ~
$


************************************************************************************************************************************************
*********************************************Node-3 To join swarm as manager directly********************************************************************
************************************************************************************************************************************************

//Right now Node-3 is part of the swarm as a worker. You cannot join as a manager directly if you are already part of the swarm.
//So first leave the swarm.

[node3] (local) root@192.168.0.46 ~
$ docker swarm leave
Node left the swarm.
[node3] (local) root@192.168.0.46 ~
$ 

//Now go to node-1 and get token to make a node join as a manager to swarm.

[node1] (local) root@192.168.0.48 ~
$ docker swarm join-token manager
To add a manager to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-19ipkr990g4rajyictoz6czytxh9c4ua5g2xv0m9q24p73ax2n-6rqttwoqpuid6xkiq9qqt9rw0 192.168.0.48:2377

[node1] (local) root@192.168.0.48 ~
$ 

//Now go to node-3 and type the above command.
[node3] (local) root@192.168.0.46 ~
$ docker swarm join --token SWMTKN-1-19ipkr990g4rajyictoz6czytxh9c4ua5g2xv0m9q24p73ax2n-6rqttwoqpuid6xkiq9qqt9rw0 192.168.0.48:2377
This node joined a swarm as a manager.
[node3] (local) root@192.168.0.46 ~
$ 

//You can do a "docker node ls" from node-3 since it is a manager now.
[node3] (local) root@192.168.0.46 ~
$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
85wy3so1zksl8zdh12jf65s13     node1               Ready               Active              Leader              19.03.4
iqcag9pn0sfvzrghzno0su04e     node2               Ready               Active              Reachable           19.03.4
f0e21cirlqfj7undr2v68brd7     node3               Down                Active                                  19.03.4
sjmctrxpno0gj06j56xy654ir *   node3               Ready               Active              Reachable           19.03.4
[node3] (local) root@192.168.0.46 ~
$ 

//Now node-3 is a manager. '*' beside the ID represents that you are currently in that node. Also the manager status is shown as reachable.


***********************************************************************************************************************************************
**************************************************Create an alpine service with 3 replicas on the 3 nodes**********************************
***********************************************************************************************************************************************

[node1] (local) root@192.168.0.48 ~
$ docker service create --replicas 3 alpine ping 8.8.8.8
z84mq7uf6knp7g35cvk7mrkbu
overall progress: 3 out of 3 tasks 
1/3: running   [==================================================>] 
2/3: running   [==================================================>] 
3/3: running   [==================================================>] 
verify: Service converged 
[node1] (local) root@192.168.0.48 ~
$ 

//List all services
[node1] (local) root@192.168.0.48 ~
$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
z84mq7uf6knp        hungry_brown        replicated          3/3                 alpine:latest       
[node1] (local) root@192.168.0.48 ~
$ 


*********************************************************************************************************************************************************
**********************************************List services, containers running on nodes
**********************************************************************************************************************************************************


//List current node containers
[node1] (local) root@192.168.0.48 ~
$ docker node ps
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                ERROR               PORTS
sdxzya8gn3h3        hungry_brown.1      alpine:latest       node1               Running             Running about a minute ago                       
[node1] (local) root@192.168.0.48 ~
$ 

//To list containers on a specific node
$ docker node ps node2
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
kxnm5w5kg2zh        hungry_brown.2      alpine:latest       node2               Running             Running 8 minutes ago                       
[node1] (local) root@192.168.0.48 ~
$


//List containers running on current node

[node1] (local) root@192.168.0.48 ~
$ docker container ls
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
ef9d321bff62        alpine:latest       "ping 8.8.8.8"      10 minutes ago      Up 10 minutes                           hungry_brown.1.sdxzya8gn3h39l3b1zad2lbxg
[node1] (local) root@192.168.0.48 ~
$ 


*******************************************************************************************************************************************************************
****************************************************Get info of a service****************************************************
command: "docker service ps <service_id>"
*******************************************************************************************************************************************************************
[node1] (local) root@192.168.0.48 ~
$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
z84mq7uf6knp        hungry_brown        replicated          3/3                 alpine:latest  

[node1] (local) root@192.168.0.48 ~
$ docker service ps z84mq7uf6knp
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
sdxzya8gn3h3        hungry_brown.1      alpine:latest       node1               Running             Running 13 minutes ago                       
kxnm5w5kg2zh        hungry_brown.2      alpine:latest       node2               Running             Running 13 minutes ago                       
w0326949r4rr        hungry_brown.3      alpine:latest       node3               Running             Running 13 minutes ago                       
[node1] (local) root@192.168.0.48 ~
$ 



*******************************************************************************************************************************************************************
************************************************************************More SWARM Features*******************************************************************************************
****************************************************************Create a Overlay Network************************************
command: "docker network create --driver overlay mydrupal"
*******************************************************************************************************************************************************************

//Create a Overlay Network
[node1] (local) root@192.168.0.48 ~
$ docker network create --driver overlay mydrupal
mqiob15o0b0dsxwvfqmga37ur
[node1] (local) root@192.168.0.48 ~
$ 
[node1] (local) root@192.168.0.48 ~
$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
802a6ee15913        bridge              bridge              local
868f3f63721a        docker_gwbridge     bridge              local
2f75c6b6b438        host                host                local
r4qmc7ee4ri7        ingress             overlay             swarm
mqiob15o0b0d        mydrupal            overlay             swarm
3d8bdc74b3b6        none                null                local
[node1] (local) root@192.168.0.48 ~
$ 

*******************************************************************************************************************************************************************
********************************************************Create a Postgres service inside the "mydrupal" network************************************
Command: "docker service create --name psql --network mydrupal -e POSTGRES_PASSWORD=mypass postgres"
*******************************************************************************************************************************************************************
//Create a Postgres service
[node1] (local) root@192.168.0.48 ~
$ docker service create --name psql --network mydrupal -e POSTGRES_PASSWORD=mypass postgres
pqg4p7nl5og19i747l2f9v7q8
overall progress: 1 out of 1 tasks 
1/1: running   [==================================================>] 
verify: Service converged 
[node1] (local) root@192.168.0.48 ~
$ 


[node1] (local) root@192.168.0.48 ~
$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
z84mq7uf6knp        hungry_brown        replicated          3/3                 alpine:latest       
pqg4p7nl5og1        psql                replicated          1/1                 postgres:latest     
[node1] (local) root@192.168.0.48 ~
$ 


[node1] (local) root@192.168.0.48 ~
$ docker service ps psql
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
or4kzh898gtx        psql.1              postgres:latest     node3               Running             Running 4 minutes ago                       
[node1] (local) root@192.168.0.48 ~
$ 


*******************************************************************************************************************************************************************
***********************************************************Create a drupal service inside the "mydrupal" network******************************************************************************************
*******************************************************************************************************************************************************************
[node1] (local) root@192.168.0.48 ~
$ docker service create --name drupal --network mydrupal -p 80:80 drupal
unoosbz9uei4tww9qvajbexne
overall progress: 1 out of 1 tasks 
1/1: running   [==================================================>] 
verify: Service converged 
[node1] (local) root@192.168.0.48 ~
$ 


[node1] (local) root@192.168.0.48 ~
$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
unoosbz9uei4        drupal              replicated          1/1                 drupal:latest       *:80->80/tcp
z84mq7uf6knp        hungry_brown        replicated          3/3                 alpine:latest       
pqg4p7nl5og1        psql                replicated          1/1                 postgres:latest     
[node1] (local) root@192.168.0.48 ~
$ 

$ docker service ps drupal
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
60myknbn5iki        drupal.1            drupal:latest       node1               Running             Running 2 minutes ago                       
[node1] (local) root@192.168.0.48 ~
$ 


[node1] (local) root@192.168.0.48 ~
$ docker service ps psql
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
or4kzh898gtx        psql.1              postgres:latest     node3               Running             Running 12 minutes ago                       
[node1] (local) root@192.168.0.48 ~
$ 

**********************************************************************************************************************************************************************
**************************************************************Two Services are running on different nodes***********************************************************************************************
The above two services drupal and psql are running on different nodes. 
"drupal" on "node1"
"psql" on "node3"

But both services can communicate with eachother with service names.. This is advantage of overlay network.
**********************************************************************************************************************************************************************

**********************************************************************************************************************************************************************
***************************************************************************Routing Mesh*******************************************************************************************
**********************************************************************************************************************************************************************
Routing Mesh load balances services across the nodes.
Even if a service is running on node1 and listening on port80, all the other 3 nodes listen on port80 and reroute the traffic to the appropriate container running the service on the node1.
This is taken care by routing mesh.
Uses VIP - Virtual IP Address
Routing mesh operates at IP and port layer. Doesnt work at the DNS layer.
Doesnt work on layer-4

**********************************************************************************************************************************************************************
**********************************************************************Create a elasticsearch service with 3 replicas***********************************************************************************
**********************************************************************************************************************************************************************

[node1] (local) root@192.168.0.33 ~
$ docker service create --name search --replicas 3 -p 9200:9200 elasticsearch:2
fp1s39hg86q0ig445w64dpjm3
overall progress: 3 out of 3 tasks 
1/3: running   [==================================================>] 
2/3: running   [==================================================>] 
3/3: running   [==================================================>] 
verify: Service converged 


[node1] (local) root@192.168.0.33 ~
$ docker service ps search
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
k7xqm1wm899n        search.1            elasticsearch:2     node1               Running             Running 31 seconds ago                       
johghg188j09        search.2            elasticsearch:2     node2               Running             Running 31 seconds ago                       
yp0bs29ls3xm        search.3            elasticsearch:2     node3               Running             Running 31 seconds ago                       
[node1] (local) root@192.168.0.33 ~
$ 



**************************************Now "search" service is running on 9200. Curl "node1ipaddress:9200" multiple times**************************************
***************************************You will notice that the name field in the json response is different each time. This is the routing mesh VIP (Virtual IP) load balancing the traffic and routing it to different tasks each time in different nodes**********************************************************************


[node1] (local) root@192.168.0.33 ~
$ curl 192.168.0.33:9200
{
  "name" : "Hero for Hire",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "u7frc6AER5Sm4FlSAJkv2g",
  "version" : {
    "number" : "2.4.6",
    "build_hash" : "5376dca9f70f3abef96a77f4bb22720ace8240fd",
    "build_timestamp" : "2017-07-18T12:17:44Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.4"
  },
  "tagline" : "You Know, for Search"
}
[node1] (local) root@192.168.0.33 ~
$ curl 192.168.0.33:9200
{
  "name" : "Abraham Cornelius",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "HRSZz_PLT5mMpPys5CSs-A",
  "version" : {
    "number" : "2.4.6",
    "build_hash" : "5376dca9f70f3abef96a77f4bb22720ace8240fd",
    "build_timestamp" : "2017-07-18T12:17:44Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.4"
  },
  "tagline" : "You Know, for Search"
}
[node1] (local) root@192.168.0.33 ~
$ 
[node1] (local) root@192.168.0.33 ~
$ curl 192.168.0.33:9200
{
  "name" : "Skids",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "N1r6yZTeSfWfbcYqhnGcjw",
  "version" : {
    "number" : "2.4.6",
    "build_hash" : "5376dca9f70f3abef96a77f4bb22720ace8240fd",
    "build_timestamp" : "2017-07-18T12:17:44Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.4"
  },
  "tagline" : "You Know, for Search"
}


******************************************************************************************************************************************************************************************************
																					MULTI SERVICE VOTING APP
******************************************************************************************************************************************************************************************************
# Assignment: Create A Multi-Service Multi-Node Web App

## Goal: create networks, volumes, and services for a web-based "cats vs. dogs" voting app.
Here is a basic diagram of how the 5 services will work:

![diagram](./architecture.png)
- All images are on Docker Hub, so you should use editor to craft your commands locally, then paste them into swarm shell (at least that's how I'd do it)
- a `backend` and `frontend` overlay network are needed. Nothing different about them other then that backend will help protect database from the voting web app. (similar to how a VLAN setup might be in traditional architecture)
- The database server should use a named volume for preserving data. Use the new `--mount` format to do this: `--mount type=volume,source=db-data,target=/var/lib/postgresql/data`

### Services (names below should be service names)
- vote
    - bretfisher/examplevotingapp_vote:before
    - web front end for users to vote dog/cat
    - ideally published on TCP 80. Container listens on 80
    - on frontend network
    - 2+ replicas of this container

- redis
    - redis:3.2
    - key/value storage for incoming votes
    - no public ports
    - on frontend network
    - 1 replica NOTE VIDEO SAYS TWO BUT ONLY ONE NEEDED

- worker
    - bretfisher/examplevotingapp_worker:java
    - backend processor of redis and storing results in postgres
    - no public ports
    - on frontend and backend networks
    - 1 replica

- db
    - postgres:9.4
    - one named volume needed, pointing to /var/lib/postgresql/data
    - on backend network
    - 1 replica

- result
    - bretfisher/examplevotingapp_result
    - web app that shows results
    - runs on high port since just for admins (lets imagine)
    - so run on a high port of your choosing (I choose 5001), container listens on 80
    - on backend network
    - 1 replica

**************************************************************************************************************************************************************************************************
**********************************************************************************
//Create two networks - backend and frontend
**********************************************************************************
[node1] (local) root@192.168.0.33 ~
$ docker network create -d overlay backend
qqxbraidptnme6zej7dyzudff
[node1] (local) root@192.168.0.33 ~
$ docker network create -d overlay frontend
w2mtt7mvzv2zgwkgu0gxglaxe
[node1] (local) root@192.168.0.33 ~
$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
qqxbraidptnm        backend             overlay             swarm
4c05935f0478        bridge              bridge              local
2b87f6dab56f        docker_gwbridge     bridge              local
w2mtt7mvzv2z        frontend            overlay             swarm
96c73a141807        host                host                local
00bncz9r8ane        ingress             overlay             swarm
5d665e631ce7        none                null                local
[node1] (local) root@192.168.0.33 ~
$ 


****************************************************************
Create all the services

Commands:
docker service create --name vote -p 80:80 --network frontend --replicas 2 bretfisher/examplevotingapp_vote
docker service create --name redis  --network frontend redis:3.2
docker service create --name worker --network frontend --network backend bretfisher/examplevotingapp_worker:java
docker service create --name db --network backend --mount type=volume,source=db-data,target=/var/lib/postgresql/data postgres:9.4
docker service create --name result --network backend -p 5001:80 bretfisher/examplevotingapp_result

-------------------------------------------------
[node2] (local) root@192.168.0.32 ~
$ docker service create --name vote -p 80:80 --network frontend --replicas 2 bretfisher/examplevotingapp_vote
telgn8c09toxnca2ak67sk9zl
overall progress: 2 out of 2 tasks 
1/2: running   
2/2: running   
verify: Service converged 
-------------------------------------------------
[node2] (local) root@192.168.0.32 ~
$ docker service create --name redis  --network frontend redis:3.2
3ina81cqfbz8ooc7qbry652ge
overall progress: 1 out of 1 tasks 
1/1: running   
verify: Service converged 
-------------------------------------------------
[node2] (local) root@192.168.0.32 ~
$ docker service create --name worker --network frontend --network backend bretfisher/examplevotingapp_worker:java
2uvxclokjv44px3jjoaiwo5t7
overall progress: 1 out of 1 tasks 
1/1: running   
verify: Service converged 
-------------------------------------------------
[node2] (local) root@192.168.0.32 ~
$ docker service create --name db --network backend --mount type=volume, source=db-data, target=/var/lib/postgresql/data postgres:9.4
invalid argument "type=volume," for "--mount" flag: invalid field '' must be a key=value pair
See 'docker service create --help'.
[node2] (local) root@192.168.0.32 ~

$ docker service create --name db --network backend --mount type=volume,source=db-data,target=/var/lib/postgresql/data postgres:9.4
7jpw3l51y2koposltsz189yt8
overall progress: 1 out of 1 tasks 
1/1: running   
verify: Service converged 
-------------------------------------------------
[node2] (local) root@192.168.0.32 ~
$ docker service create --name result --network backend -p 5001:80 bretfisher/examplevotingapp_result
jfs07tyc2r844dfiu6ex1lnz8
overall progress: 1 out of 1 tasks 
1/1: running   
verify: Service converged 
[node2] (local) root@192.168.0.32 ~
$ 
-------------------------------------------------
-------------------------------------------------


[node2] (local) root@192.168.0.32 ~
$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                                       PORTS
7jpw3l51y2ko        db                  replicated          1/1                 postgres:9.4                                
3ina81cqfbz8        redis               replicated          1/1                 redis:3.2                                   
jfs07tyc2r84        result              replicated          1/1                 bretfisher/examplevotingapp_result:latest   *:5001->80/tcp
telgn8c09tox        vote                replicated          2/2                 bretfisher/examplevotingapp_vote:latest     *:80->80/tcp
2uvxclokjv44        worker              replicated          1/1                 bretfisher/examplevotingapp_worker:java     
[node2] (local) root@192.168.0.32 ~
$ 


[node2] (local) root@192.168.0.32 ~
$ docker service ps db
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
r4awgx84tnva        db.1                postgres:9.4        node2               Running             Running 3 minutes ago                       
[node2] (local) root@192.168.0.32 ~
$ docker service ps redis
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
tsodomlmb6c1        redis.1             redis:3.2           node3               Running             Running 10 minutes ago                       
[node2] (local) root@192.168.0.32 ~
$ docker service ps result
ID                  NAME                IMAGE                                       NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
zltmto8i0zvx        result.1            bretfisher/examplevotingapp_result:latest   node3               Running             Running 3 minutes ago                       
[node2] (local) root@192.168.0.32 ~
$ docker service ps vote
ID                  NAME                IMAGE                                     NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
rvlps8ocy3uz        vote.1              bretfisher/examplevotingapp_vote:latest   node2               Running             Running 10 minutes ago                       
ngomj1nmzysc        vote.2              bretfisher/examplevotingapp_vote:latest   node1               Running             Running 10 minutes ago                       
[node2] (local) root@192.168.0.32 ~
$ docker service ps worker
ID                  NAME                IMAGE                                     NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
szftb6bqx5c0        worker.1            bretfisher/examplevotingapp_worker:java   node1               Running             Running 10 minutes ago                       
[node2] (local) root@192.168.0.32 ~
$



**************************************************************************************************************************************************************************************************
**************************************************************************************************************************************************************************************************
************************************************************************Swarm Stacks****"docker stack command"********************************************************************
**************************************************************************************************************************************************************************************************
--Stack is a production grade compose.
--There is no "build" here. Only "deploy" since stack operates on swarm production and not in ci-cd
Stacks accept compose files as the declarative definition for services, networks and volumes
We use "docker stack deploy" instead of "docker service create"

*************************************Example compose yml file*******************************************************************

version: '3.4'
services:
    redis:
        image: redis
        healthcheck:
            test: ["CMD", "redis-healthcheck"]
            interval: 5s
            timeout: 5s
            retries: 3
            start_period: 60s
        deploy:
            replicas: 1
            update_config:
              failure_action: rollback
        configs:
            - source: redis-healthcheck
              target: /usr/local/bin/redis-healthcheck
              mode: 0555
        networks:
            - frontend
    db:
        image: postgres:9.6
        healthcheck:
            test: ["CMD", "postgres-healthcheck"]
            interval: 5s
            timeout: 5s
            retries: 3
            start_period: 60s
        deploy:
            replicas: 1
            update_config:
              failure_action: rollback
        configs:
            - source: postgres-healthcheck
              target: /usr/local/bin/postgres-healthcheck
              mode: 0555
        volumes:
            - db-data:/var/lib/postgresql/data
        networks:
            - backend
    vote:
        image: bretfisher/examplevotingapp_vote
        ports:
            - '5000:80'
        healthcheck: 
            test: ["CMD-SHELL", "curl -f http://localhost || exit 1"]
            interval: 5s
            timeout: 5s
            retries: 3
            start_period: 20s
        deploy:
            replicas: 2
            update_config:
                failure_action: rollback
                order: start-first
        networks:
            - frontend
    result:
        image: bretfisher/examplevotingapp_result
        ports:
            - '5001:80'
        networks:
            - backend
    worker:
        image: bretfisher/examplevotingapp_worker:java
        networks:
            - frontend
            - backend
        deploy:
          replicas: 2

networks:
    frontend:
    backend:
volumes:
    db-data:

configs:
  redis-healthcheck:
    file: ./redis-healthcheck
  postgres-healthcheck:
    file: ./postgres-healthcheck


**************************************************************************************************************************************************************************************************
command: docker stack deploy -c example-voting-app-stack.yml voteapp
**************************************************************************************************************************************************************************************************
"docker stack commands"
docker stack ls
docker stack ps <stack_name>
docker stack services <stack_name>
docker stack deploy -c example-voting-app-stack.yml voteapp
*************************************************************************
[node1] (local) root@192.168.0.33 ~
$ docker stack --help

Usage:  docker stack [OPTIONS] COMMAND

Manage Docker stacks

Options:
      --orchestrator string   Orchestrator to use (swarm|kubernetes|all)

Commands:
  deploy      Deploy a new stack or update an existing stack
  ls          List stacks
  ps          List the tasks in the stack
  rm          Remove one or more stacks
  services    List the services in the stack

Run 'docker stack COMMAND --help' for more information on a command.
[node1] (local) root@192.168.0.33 ~
$ 




[node1] (local) root@192.168.0.33 ~
$ ls
example-voting-app-stack.yml  postgres-healthcheck          redis-healthcheck
[node1] (local) root@192.168.0.33 ~
$ 

[node1] (local) root@192.168.0.33 ~
$ docker stack deploy -c example-voting-app-stack.yml voteapp
Creating network voteapp_frontend
Creating network voteapp_backend
Creating config voteapp_redis-healthcheck
Creating config voteapp_postgres-healthcheck
Creating service voteapp_redis
Creating service voteapp_db
Creating service voteapp_vote
Creating service voteapp_result
Creating service voteapp_worker
[node1] (local) root@192.168.0.33 ~
$


[node1] (local) root@192.168.0.33 ~
$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
8f4e206b7f12        bridge              bridge              local
6cadb71111cd        docker_gwbridge     bridge              local
1a28ab397be1        host                host                local
dj61etn509ls        ingress             overlay             swarm
2d05a7ce1d90        none                null                local
p740iqf4gl05        voteapp_backend     overlay             swarm
yzfu6s7jy9qh        voteapp_frontend    overlay             swarm
[node1] (local) root@192.168.0.33 ~
$ 


[node1] (local) root@192.168.0.33 ~
$ docker stack ls
NAME                SERVICES            ORCHESTRATOR
voteapp             5                   Swarm
[node1] (local) root@192.168.0.33 ~
$ docker stack ps voteapp
ID                  NAME                IMAGE                                       NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
baxxwgx8zsul        voteapp_worker.1    bretfisher/examplevotingapp_worker:java     node3               Running             Running 4 minutes ago                       
tw6ssxdxfc4s        voteapp_result.1    bretfisher/examplevotingapp_result:latest   node1               Running             Running 4 minutes ago                       
p54zzqu1crjz        voteapp_vote.1      bretfisher/examplevotingapp_vote:latest     node3               Running             Running 4 minutes ago                       
nyu7vz8s31mw        voteapp_db.1        postgres:9.6                                node2               Running             Running 4 minutes ago                       
s9fer1odg60m        voteapp_redis.1     redis:latest                                node1               Running             Running 4 minutes ago                       
nbsg1nfdadsq        voteapp_worker.2    bretfisher/examplevotingapp_worker:java     node2               Running             Running 4 minutes ago                       
09jckdbf4y40        voteapp_vote.2      bretfisher/examplevotingapp_vote:latest     node2               Running             Running 4 minutes ago                       
[node1] (local) root@192.168.0.33 ~
$ 

[node1] (local) root@192.168.0.33 ~
$ docker stack services voteapp
ID                  NAME                MODE                REPLICAS            IMAGE                                       PORTS
fxb62i3erbjd        voteapp_worker      replicated          2/2                 bretfisher/examplevotingapp_worker:java     
hxcmdv95lvxk        voteapp_db          replicated          1/1                 postgres:9.6                                
hxpnnsajel8q        voteapp_vote        replicated          2/2                 bretfisher/examplevotingapp_vote:latest     *:5000->80/tcp
r5q6l6xfwuzf        voteapp_redis       replicated          1/1                 redis:latest                                
sjf29hb3vx81        voteapp_result      replicated          1/1                 bretfisher/examplevotingapp_result:latest   *:5001->80/tcp
[node1] (local) root@192.168.0.33 ~
$ 


**************************************************************************************************************************************************************************************************
**************************************************************************************************************************************************************************************************
																				Using Secrets in Swarm
**************************************************************************************************************************************************************************************************
commands:
docker secret create psql_user psql_user.txt
echo "myDBPassword" | docker secret create psql_pass -
docker secret ls
docker secret inspect psql_pass
**************************************************************************************************************************************************************************************************

[node1] (local) root@192.168.0.33 ~
$ docker secret --help

Usage:  docker secret COMMAND

Manage Docker secrets

Commands:
  create      Create a secret from a file or STDIN as content
  inspect     Display detailed information on one or more secrets
  ls          List secrets
  rm          Remove one or more secrets

Run 'docker secret COMMAND --help' for more information on a command.
[node1] (local) root@192.168.0.33 ~
$ 
[node1] (local) root@192.168.0.33 ~
$ ls
example-voting-app-stack.yml  postgres-healthcheck          psql_user.txt                 redis-healthcheck
[node1] (local) root@192.168.0.33 ~
$
[node1] (local) root@192.168.0.33 ~
$ docker secret create psql_user psql_user.txt
rc3jrt0mlt1hjrlbjds91skxh
[node1] (local) root@192.168.0.33 ~
$ 

[node1] (local) root@192.168.0.33 ~
$ echo "myDBPassword" | docker secret create psql_pass -
ukzisdm6ckigllxggt603yfbz
[node1] (local) root@192.168.0.33 ~
$ 

[node1] (local) root@192.168.0.33 ~
$ docker secret ls
ID                          NAME                DRIVER              CREATED             UPDATED
ukzisdm6ckigllxggt603yfbz   psql_pass                               20 seconds ago      20 seconds ago
rc3jrt0mlt1hjrlbjds91skxh   psql_user                               2 minutes ago       2 minutes ago
[node1] (local) root@192.168.0.33 ~
$ 

[node1] (local) root@192.168.0.33 ~
$ docker secret inspect psql_pass
[
    {
        "ID": "ukzisdm6ckigllxggt603yfbz",
        "Version": {
            "Index": 81
        },
        "CreatedAt": "2019-11-04T08:33:28.449246925Z",
        "UpdatedAt": "2019-11-04T08:33:28.449246925Z",
        "Spec": {
            "Name": "psql_pass",
            "Labels": {}
        }
    }
]
[node1] (local) root@192.168.0.33 ~
$ 

**************************************************************************************************************************************************************************************************
Create a docker service using secret
**************************************************************************************************************************************************************************************************
command: 
docker service create --name psql --secret psql_user --secret psql_pass -e 	POSTGRES_PASSWORD_FILE=/run/secrets/psql_pass -e POSTGRESS_USER_FILE=/run/secrets/psql_user postgres
**************************************************************************************************************************************************************************************************
[node1] (local) root@192.168.0.33 ~
$ docker service create --name psql --secret psql_user --secret psql_pass -e POSTGRES_PASSWORD_FILE=/run/secrets/psql_pass -e POSTGRESS_USER_FILE=/run/secrets/psql_user postgres
956zu88jzjg6ob8bf7mdkwm5c
overall progress: 1 out of 1 tasks 
1/1: running   [==================================================>] 
verify: Service converged 
[node1] (local) root@192.168.0.33 ~
$ 
[node1] (local) root@192.168.0.33 ~
$ docker service ps psql
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
t6pemry4d84e        psql.1              postgres:latest     node1               Running             Running 2 minutes ago                       
[node1] (local) root@192.168.0.33 ~
$ docker container ls
CONTAINER ID        IMAGE                                       COMMAND                  CREATED             STATUS                    PORTS               NAMES
eb87a866c768        postgres:latest                             "docker-entrypoint.s…"   3 minutes ago       Up 3 minutes              5432/tcp            psql.1.t6pemry4d84ex
rnoksaj1znkd
91d98c20a433        bretfisher/examplevotingapp_result:latest   "node server.js"         46 minutes ago      Up 46 minutes             80/tcp              voteapp_result.1.tw6
ssxdxfc4sj76vwkgdh43ek
968f005de298        redis:latest                                "docker-entrypoint.s…"   47 minutes ago      Up 46 minutes (healthy)   6379/tcp            voteapp_redis.1.s9fe
r1odg60minydc02bpatpf
[node1] (local) root@192.168.0.33 ~
$ docker exec -ti eb87a866c768 /bin/bash
root@eb87a866c768:/# cd /run/secrets
root@eb87a866c768:/run/secrets# ls
psql_pass  psql_user
root@eb87a866c768:/run/secrets# cat psql_pass
myDBPassword
root@eb87a866c768:/run/secrets# cat psql_user
mypsqluser
root@eb87a866c768:/run/secrets# 


**********When you login to the container running psql service, you can view the secrets in plain text.

**************************************************************************************************************************************************************************************************
Using Stack with secrets
**************************************************************************************************************************************************************************************************

Docker-compose.yml file
version: "3.1"

services:
  psql:
    image: postgres
    secrets:
      - psql_user
      - psql_password
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/psql_password
      POSTGRES_USER_FILE: /run/secrets/psql_user

secrets:
  psql_user:
    file: ./psql_user.txt
  psql_password:
    file: ./psql_password.txt

**************************************************************************************************************************************************************************************************
command:
docker stack deploy -c docker-compose.yml mydb

[node1] (local) root@192.168.0.33 ~
$ docker stack deploy -c docker-compose.yml mydb
Creating secret mydb_psql_user
Creating secret mydb_psql_password
Creating service mydb_psql
[node1] (local) root@192.168.0.33 ~
$ docker secret ls
ID                          NAME                 DRIVER              CREATED             UPDATED
mdx68ccn4q0b707ej1tdsz6ac   mydb_psql_password                       6 seconds ago       6 seconds ago
2u9jg63miu8psbnpdl59x8two   mydb_psql_user                           6 seconds ago       6 seconds ago
ukzisdm6ckigllxggt603yfbz   psql_pass                                23 minutes ago      23 minutes ago
rc3jrt0mlt1hjrlbjds91skxh   psql_user                                25 minutes ago      25 minutes ago
[node1] (local) root@192.168.0.33 ~
$ 

[node1] (local) root@192.168.0.33 ~
$ docker stack rm mydb
Removing service mydb_psql
Removing secret mydb_psql_user
Removing secret mydb_psql_password
Removing network mydb_default
[node1] (local) root@192.168.0.33 ~
$ 

Removing a stack deletes secrets of that stack


**************************************************************************************************************************************************************************************************
                                                                              Docker service updates 
**************************************************************************************************************************************************************************************************
//Update your base image to a newer version
command: "docker service update --image myapp:1.2.1 <servicename>"

//update multiple things in a single command in a service - add an environment variable and remove a port
command: "docker service update --env-add NODE_ENV=production --publish-rm 8080

//Change number of replicas of two services
command: "docker service scale web=8 api=6"

//create a service
docker service create -p 8088:80 --name web nginx:1.13.7

//scale up the service
docker service scale web=5

//do a rolling update of the service "web" by changing the image of nginx. here we change the image
docker service update --image nginx:1.13.6 web

//change the published port
docker service update --publish-rm 8088 --publish-add 9090:80

//tip- In real world scenario you will find some swarm nodes taking more load and other nodes with less load. If you want to move a service to lighter node, then do an empty service update. Then swarm will move the service to the node that has low load. Swam by default may not distribute the load evenly across nodes. 

**************************************************************************************************************************************************************************************************
																				Health Checks
**************************************************************************************************************************************************************************************************

Added in 1.12
Supported in Dockerfile, Compose Yaml, docker run and swarm services.









































